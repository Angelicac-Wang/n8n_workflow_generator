{
  "template_id": "10887",
  "llm_response": {
    "mode": "create_workflow",
    "reply": "Here's a workflow to implement semantic caching for chat queries.",
    "workflowPlan": {
      "name": "Semantic Caching for Chat Queries",
      "nodes": [
        {
          "id": "1",
          "label": "Receive Question",
          "nodeType": "n8n-nodes-base.webhook",
          "typeVersion": 1,
          "params": {
            "httpMethod": "POST",
            "path": "receive-question"
          },
          "position": {
            "x": 200,
            "y": 300
          }
        },
        {
          "id": "2",
          "label": "Vector Search in Redis",
          "nodeType": "n8n-nodes-base.httpRequest",
          "typeVersion": 1,
          "params": {
            "url": "http://your-redis-instance/vector-search",
            "method": "POST",
            "bodyParameters": [
              {
                "name": "query",
                "value": "={{$json[\"body\"][\"question\"]}}"
              }
            ]
          },
          "position": {
            "x": 400,
            "y": 300
          }
        },
        {
          "id": "3",
          "label": "Check Cache Hit",
          "nodeType": "n8n-nodes-base.if",
          "typeVersion": 1,
          "params": {
            "conditions": {
              "boolean": [
                {
                  "value1": "={{$json[\"cacheHit\"]}}",
                  "operation": "isTrue"
                }
              ]
            }
          },
          "position": {
            "x": 600,
            "y": 300
          }
        },
        {
          "id": "4",
          "label": "Return Cached Response",
          "nodeType": "n8n-nodes-base.respondText",
          "typeVersion": 1,
          "params": {
            "responseText": "={{$json[\"cachedResponse\"]}}"
          },
          "position": {
            "x": 800,
            "y": 200
          }
        },
        {
          "id": "5",
          "label": "Call LLM",
          "nodeType": "n8n-nodes-base.httpRequest",
          "typeVersion": 1,
          "params": {
            "url": "https://api.openai.com/v1/engines/davinci-codex/completions",
            "method": "POST",
            "authentication": "predefinedCredentialType",
            "bodyParameters": [
              {
                "name": "prompt",
                "value": "={{$json[\"body\"][\"question\"]}}"
              }
            ]
          },
          "position": {
            "x": 800,
            "y": 400
          }
        },
        {
          "id": "6",
          "label": "Cache New Answer",
          "nodeType": "n8n-nodes-base.httpRequest",
          "typeVersion": 1,
          "params": {
            "url": "http://your-redis-instance/cache-answer",
            "method": "POST",
            "bodyParameters": [
              {
                "name": "question",
                "value": "={{$json[\"body\"][\"question\"]}}"
              },
              {
                "name": "answer",
                "value": "={{$json[\"response\"]}}"
              }
            ]
          },
          "position": {
            "x": 1000,
            "y": 400
          }
        },
        {
          "id": "7",
          "label": "Return New Response",
          "nodeType": "n8n-nodes-base.respondText",
          "typeVersion": 1,
          "params": {
            "responseText": "={{$json[\"response\"]}}"
          },
          "position": {
            "x": 1200,
            "y": 400
          }
        }
      ],
      "connections": [
        {
          "from": "1",
          "to": "2"
        },
        {
          "from": "2",
          "to": "3"
        },
        {
          "from": "3",
          "to": "4",
          "outputIndex": 0
        },
        {
          "from": "3",
          "to": "5",
          "outputIndex": 1
        },
        {
          "from": "5",
          "to": "6"
        },
        {
          "from": "6",
          "to": "7"
        }
      ]
    }
  },
  "usage": {
    "prompt_tokens": 1152,
    "completion_tokens": 993,
    "total_tokens": 2145
  },
  "error": null,
  "generated_at": "2026-01-11T16:38:43.743482"
}