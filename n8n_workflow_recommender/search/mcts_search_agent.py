#!/usr/bin/env python3
"""
MCTS æœç´¢ä»£ç†

é©é… n8n çš„ taxonomyï¼Œä½¿ç”¨ MCTS ç®—æ³•é€²è¡Œæœç´¢ã€‚
"""

import json
import math
import numpy as np
import os
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
from sentence_transformers import SentenceTransformer, util
import torch

# é¿å… huggingface tokenizers çš„è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"


class MCTSNode:
    """MCTS ç¯€é»"""
    def __init__(self, name, parent=None, taxonomy_data=None):
        self.name = name
        self.parent = parent
        self.taxonomy_data = taxonomy_data
        self.children = []
        self.visits = 0
        self.total_reward = 0.0
    
    def is_fully_expanded(self):
        if 'children' not in self.taxonomy_data or not self.taxonomy_data['children']:
            return True
        return len(self.children) == len(self.taxonomy_data['children'])
    
    def select_best_child(self, c_param=1.414):
        """UCT å…¬å¼å¯¦ç¾"""
        best_score, best_child = -float('inf'), None
        for child in self.children:
            exploit = child.total_reward / (child.visits + 1e-6)
            explore = math.sqrt(math.log(self.visits + 1) / (child.visits + 1e-6))
            score = exploit + c_param * explore
            if score > best_score:
                best_score, best_child = score, child
        return best_child


class TaxonomySearchAgent:
    """
    Taxonomy æœç´¢ä»£ç†ï¼ˆMCTSï¼‰
    
    ä½¿ç”¨ MCTS ç®—æ³•åœ¨ n8n taxonomy ä¸­æœç´¢ç›¸é—œç¯€é»ã€‚
    """
    
    def __init__(self, taxonomy_path: str, model_name: str = "paraphrase-multilingual-mpnet-base-v2"):
        """
        åˆå§‹åŒ–æœç´¢ä»£ç†
        
        Args:
            taxonomy_path: MCTS æ ¼å¼çš„ taxonomy JSON æª”æ¡ˆè·¯å¾‘
            model_name: SentenceTransformer æ¨¡å‹åç¨±
        """
        print("PHASE 1A: Initializing Taxonomy Search Agent (MCTS)...")
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model = SentenceTransformer(model_name, device=device)
        # âœ… å„ªåŒ–ï¼šç·©å­˜ use_case embeddingsï¼Œé¿å…é‡è¤‡è¨ˆç®—
        self.use_case_embedding_cache = {}  # {use_case_text: embedding}
        self._prepare_data(taxonomy_path)
        print(" - MCTS Taxonomy data prepared.")
    
    def _is_leaf_node(self, node_content: Dict) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºè‘‰å­ç¯€é»ï¼ˆåŒ…å« Nodes æˆ– mapped_nodesï¼‰"""
        if not isinstance(node_content, dict):
            return False
        # å¦‚æœæœ‰ Nodes æˆ– mapped_nodesï¼Œä¸”ä¸æ˜¯ç©ºçš„ï¼Œå‰‡æ˜¯è‘‰å­ç¯€é»
        has_nodes = bool(node_content.get("Nodes") or node_content.get("mapped_nodes"))
        # æˆ–è€…æ˜ç¢ºæ¨™è¨˜ç‚º is_leaf
        is_leaf_flag = node_content.get('is_leaf', False)
        return has_nodes or is_leaf_flag
    
    def _prepare_data(self, taxonomy_path: str):
        """æº–å‚™ taxonomy æ•¸æ“š"""
        taxonomy_path = Path(taxonomy_path)
        
        if not taxonomy_path.exists():
            raise FileNotFoundError(f"Taxonomy æª”æ¡ˆä¸å­˜åœ¨: {taxonomy_path}")
        
        with open(taxonomy_path, 'r', encoding='utf-8') as f:
            raw_taxonomy = json.load(f)
        
        # ä¿å­˜åŸå§‹ taxonomy æ•¸æ“šï¼ˆç”¨æ–¼ category æœç´¢ï¼‰
        # æå– Taxonomy æ ¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "Taxonomy" in raw_taxonomy:
            self.raw_taxonomy_for_search = raw_taxonomy["Taxonomy"]
        else:
            self.raw_taxonomy_for_search = raw_taxonomy
        
        self.node_database = []
        texts_to_encode = []
        
        def traverse(node_name: str, node_content: Dict, current_path: List[str]):
            """éæ­¸éæ­· taxonomyï¼ˆç‚ºæ‰€æœ‰ç¯€é»ç”Ÿæˆ embeddingï¼‰"""
            full_path_list = current_path + [node_name]
            full_path_str = " -> ".join(full_path_list)
            
            description = node_content.get("Description", node_content.get("description", ""))
            mapped_nodes = node_content.get("Nodes", node_content.get("mapped_nodes", []))
            
            # æ ¸å¿ƒæè¿°ç”¨æ–¼èªç¾©åŒ¹é…ï¼ˆçµ±ä¸€æ ¼å¼ï¼Œä¸åŒ…å« Nodesï¼Œå› ç‚ºèªç¾©åŒ¹é…ä¸»è¦çœ‹è·¯å¾‘å’Œæè¿°ï¼‰
            combined_text = f"{full_path_str}: {description}"
            
            # ç‚ºæ‰€æœ‰ç¯€é»ï¼ˆåŒ…æ‹¬ä¸­é–“ç¯€é»ï¼‰ç”Ÿæˆ embedding
            texts_to_encode.append(combined_text)
            
            if self._is_leaf_node(node_content):
                # ä¿å­˜ example_use_casesï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                example_use_cases = node_content.get("example_use_cases", [])
                self.node_database.append({
                    "description": description,
                    "path_str": full_path_str,
                    "mapped_nodes": mapped_nodes,
                    "example_use_cases": example_use_cases,  # ä¿å­˜ä»¥æ”¯æŒé—œéµå­—åŒ¹é…
                    "combined_text": combined_text
                })
            else:
                # éæ­¸è™•ç†å­ç¯€é»ï¼ˆå­ç¯€é»æ˜¯ç›´æ¥ä½œç‚ºå­—å…¸çš„éµï¼Œè€Œä¸æ˜¯åœ¨ "children" éµä¸‹ï¼‰
                for child_name, child_content in node_content.items():
                    # è·³éç‰¹æ®Šéµ
                    if child_name in ["Description", "description", "Nodes", "mapped_nodes", "example_use_cases"]:
                        continue
                    # åªè™•ç†å­—å…¸é¡å‹çš„å­ç¯€é»
                    if isinstance(child_content, dict):
                        traverse(child_name, child_content, full_path_list)
        
        # æå– Taxonomy æ ¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "Taxonomy" in raw_taxonomy:
            taxonomy_root = raw_taxonomy["Taxonomy"]
        else:
            taxonomy_root = raw_taxonomy
        
        # éæ­·æ‰€æœ‰é ‚å±¤åˆ†é¡
        for root_key, root_content in taxonomy_root.items():
            traverse(root_key, root_content, [])
        
        # ç”Ÿæˆ embeddings
        print(f"   ğŸ“Š ç·¨ç¢¼ {len(texts_to_encode)} å€‹ç¯€é»æè¿°...")
        embeddings = self.model.encode(texts_to_encode, convert_to_tensor=True, show_progress_bar=True)
        self.text_embedding_map = {text: emb for text, emb in zip(texts_to_encode, embeddings)}
        
        # æ§‹å»º MCTS æ¨¹
        def build_mcts_tree(node_name: str, node_content: Dict, current_path: List[str]) -> Dict:
            """æ§‹å»º MCTS æ¨¹çµæ§‹"""
            full_path_list = current_path + [node_name]
            full_path_str = " -> ".join(full_path_list)
            is_leaf = self._is_leaf_node(node_content)
            description = node_content.get("Description", node_content.get("description", ""))
            combined_text = f"{full_path_str}: {description}"
            
            # æŸ¥æ‰¾ embeddingï¼ˆå¿…é ˆèˆ‡ traverse ä¸­ç”Ÿæˆçš„æ ¼å¼ä¸€è‡´ï¼‰
            embedding = self.text_embedding_map.get(combined_text)
            if embedding is None:
                # èª¿è©¦ï¼šè¼¸å‡ºæ‰¾ä¸åˆ° embedding çš„ç¯€é»
                print(f"   - Warning: No embedding for {full_path_str}")
            
            processed_node = {
                'embedding': embedding,
                'description': description,
                'mapped_nodes': node_content.get("Nodes", node_content.get("mapped_nodes", [])),
                'children': {},
                'is_leaf': is_leaf
            }
            
            if not is_leaf:
                # å­ç¯€é»æ˜¯ç›´æ¥ä½œç‚ºå­—å…¸çš„éµï¼Œè€Œä¸æ˜¯åœ¨ "children" éµä¸‹
                for child_name, child_content in node_content.items():
                    # è·³éç‰¹æ®Šéµ
                    if child_name in ["Description", "description", "Nodes", "mapped_nodes", "example_use_cases"]:
                        continue
                    # åªè™•ç†å­—å…¸é¡å‹çš„å­ç¯€é»
                    if isinstance(child_content, dict):
                        processed_node['children'][child_name] = build_mcts_tree(
                            child_name, child_content, full_path_list
                        )
            
            return processed_node
        
        # âœ… æ§‹å»ºè™›æ“¬æ ¹ç¯€é» "Taxonomy"ï¼Œå°‡æ‰€æœ‰é ‚å±¤åˆ†é¡ä½œç‚ºå…¶å­ç¯€é»ï¼ˆç¬¬äºŒå±¤ï¼‰
        # æå– Taxonomy æ ¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "Taxonomy" in raw_taxonomy:
            taxonomy_root_for_tree = raw_taxonomy["Taxonomy"]
        else:
            taxonomy_root_for_tree = raw_taxonomy
        
        # å‰µå»ºè™›æ“¬æ ¹ç¯€é» "Taxonomy"ï¼Œå°‡æ‰€æœ‰é ‚å±¤åˆ†é¡ï¼ˆ1-9ï¼‰ä½œç‚ºå…¶å­ç¯€é»
        virtual_root_children = {}
        for root_key, root_content in taxonomy_root_for_tree.items():
            if isinstance(root_content, dict):
                virtual_root_children[root_key] = build_mcts_tree(root_key, root_content, [])
        
        # ç‚ºè™›æ“¬æ ¹ç¯€é»å‰µå»ºä¸€å€‹æè¿°å’Œ embedding
        virtual_root_description = "n8n Taxonomy: Complete workflow automation node categories"
        virtual_root_combined_text = f"Taxonomy: {virtual_root_description}"
        
        # ç‚ºè™›æ“¬æ ¹ç¯€é»ç”Ÿæˆ embeddingï¼ˆå¦‚æœé‚„æ²’æœ‰çš„è©±ï¼‰
        if virtual_root_combined_text not in self.text_embedding_map:
            virtual_root_embedding = self.model.encode(virtual_root_combined_text, convert_to_tensor=True)
            self.text_embedding_map[virtual_root_combined_text] = virtual_root_embedding
        
        # æ§‹å»ºè™›æ“¬æ ¹ç¯€é»
        self.mcts_taxonomy_tree = {
            "Taxonomy": {
                'embedding': self.text_embedding_map.get(virtual_root_combined_text),
                'description': virtual_root_description,
                'mapped_nodes': [],
                'children': virtual_root_children,
                'is_leaf': False
            }
        }
    
    def _filter_keywords(self, keywords: Set[str], function_categories: List[str]) -> Set[str]:
        """
        éæ¿¾é—œéµå­—ï¼šç§»é™¤é¡åˆ¥åç¨±ï¼Œåªä¿ç•™æŠ€è¡“é—œéµå­—
        
        Args:
            keywords: åŸå§‹é—œéµå­—é›†åˆ
            function_categories: åŠŸèƒ½é¡åˆ¥åˆ—è¡¨
        
        Returns:
            filtered_keywords: éæ¿¾å¾Œçš„é—œéµå­—é›†åˆ
        """
        if not keywords:
            return set()
        
        # å°‡é¡åˆ¥åç¨±è½‰æ›ç‚ºå°å¯«é›†åˆï¼Œç”¨æ–¼éæ¿¾
        category_lower = {cat.lower() for cat in function_categories} if function_categories else set()
        
        # å¸¸è¦‹çš„ç„¡ç”¨é—œéµå­—ï¼ˆé¡åˆ¥åç¨±ã€åœç”¨è©ç­‰ï¼‰
        stop_words = {
            'productivity', 'collaboration', 'automation', 'intelligence',
            'ai, ml & automation intelligence', 'productivity & collaboration',
            'ml', 'ai', '&', 'and', 'or', 'the', 'a', 'an'
        }
        
        filtered = set()
        for kw in keywords:
            kw_lower = kw.lower().strip()
            
            # è·³éç©ºå­—ä¸²
            if not kw_lower:
                continue
            
            # è·³éé¡åˆ¥åç¨±
            if kw_lower in category_lower:
                continue
            
            # è·³éåœç”¨è©
            if kw_lower in stop_words:
                continue
            
            # è·³éå¤ªçŸ­çš„é—œéµå­—ï¼ˆå°‘æ–¼ 2 å€‹å­—ç¬¦ï¼‰
            if len(kw_lower) < 2:
                continue
            
            # ä¿ç•™æŠ€è¡“é—œéµå­—
            filtered.add(kw)
        
        return filtered if filtered else keywords  # å¦‚æœéæ¿¾å¾Œç‚ºç©ºï¼Œè¿”å›åŸå§‹é—œéµå­—
    
    def _calculate_category_reward(self, node: MCTSNode, categories: List[str]) -> float:
        """
        åŸºæ–¼ GPT è­˜åˆ¥çš„åŠŸèƒ½é¡åˆ¥è¨ˆç®—çå‹µ
        
        GPT çš„ categories æ˜¯ç¬¬ä¸‰å±¤çš„åç¨±ï¼ˆå¦‚ "Campaign Automation", "Calendar & Booking"ï¼‰
        éœ€è¦æª¢æŸ¥ç¯€é»è·¯å¾‘ä¸­æ˜¯å¦åŒ…å«é€™äº› category åç¨±
        """
        if not categories:
            return 0.0
        
        path_nodes = []
        curr = node
        while curr is not None:
            path_nodes.append(curr.name)
            curr = curr.parent
        
        # æ§‹å»ºå®Œæ•´è·¯å¾‘å­—ç¬¦ä¸²ï¼ˆç”¨æ–¼åŒ¹é…ï¼‰
        full_path_str = " -> ".join(reversed(path_nodes))
        full_path_lower = full_path_str.lower()
        
        # æª¢æŸ¥æ¯å€‹ GPT category æ˜¯å¦åœ¨è·¯å¾‘ä¸­
        matches = 0
        matched_categories = []
        for category in categories:
            category_lower = category.lower()
            # æª¢æŸ¥ category åç¨±æ˜¯å¦åœ¨è·¯å¾‘ä¸­
            # ä¾‹å¦‚ï¼š"Campaign Automation" æ‡‰è©²åŒ¹é… "2.1.1 Campaign Automation"
            if category_lower in full_path_lower:
                matches += 1
                matched_categories.append(category)
        
        # è¿”å›åŒ¹é…æ¯”ä¾‹
        reward = matches / len(categories) if categories else 0.0
        
        # èª¿è©¦ï¼šå¦‚æœ reward > 0ï¼Œè¼¸å‡ºåŒ¹é…ä¿¡æ¯
        if reward > 0:
            print(f"   - Category match found: {full_path_str[:60]}... matches {matched_categories}")
        
        return reward
    
    def search_with_categories(
        self,
        semantic_query: str,
        function_categories: List[str],
        extracted_keywords: Optional[Set[str]] = None,
        iterations: int = 2000,
        top_n: int = 5
    ) -> List[Dict]:
        """
        ä½¿ç”¨ GPT æå–çš„åŠŸèƒ½é¡åˆ¥ + é—œéµå­—åŒ¹é…é€²è¡Œæœç´¢
        
        Args:
            semantic_query: èªç¾©æŸ¥è©¢å­—ç¬¦ä¸²
            function_categories: GPT æå–çš„åŠŸèƒ½é¡åˆ¥åˆ—è¡¨
            extracted_keywords: æå–çš„é—œéµå­—é›†åˆ
            iterations: MCTS è¿­ä»£æ¬¡æ•¸
            top_n: è¿”å›å‰ n å€‹çµæœ
        
        Returns:
            results: æœç´¢çµæœåˆ—è¡¨
        """
        print(f" - MCTS searching with semantic_query: '{semantic_query[:40]}...'")
        print(f" - Using function categories from GPT: {function_categories}")
        if extracted_keywords:
            print(f" - Extracted keywords for matching: {extracted_keywords}")
        
        # ç”ŸæˆæŸ¥è©¢ embedding
        query_embedding = self.model.encode(semantic_query, convert_to_tensor=True)
        print(f"   - Query text: '{semantic_query[:100]}...'")
        print(f"   - Query embedding shape: {query_embedding.shape}")
        
        # âœ… ä½¿ç”¨è™›æ“¬æ ¹ç¯€é» "Taxonomy"ï¼Œå°‡æ‰€æœ‰é ‚å±¤åˆ†é¡ï¼ˆ1-9ï¼‰ä½œç‚ºç¬¬äºŒå±¤
        root_name = "Taxonomy"
        root = MCTSNode(name=root_name, taxonomy_data=self.mcts_taxonomy_tree[root_name])
        
        print(f"   - Using virtual root node: {root_name}")
        print(f"   - Second level nodes: {list(self.mcts_taxonomy_tree[root_name]['children'].keys())}")
        print(f"   - Running {iterations} MCTS iterations...")
        
        # å°å–®ä¸€æ ¹ç¯€é»é€²è¡Œ MCTS æœç´¢ï¼ˆ1222_vincent çš„æ–¹æ³•ï¼‰
        for i in range(iterations):
                # é€²åº¦è¼¸å‡ºï¼ˆä½†ä¸å½±éŸ¿ MCTS é‚è¼¯ï¼‰
                if i % 500 == 0 and i > 0:
                    print(f"   - Progress: {i}/{iterations} iterations")
                
                # MCTS æ ¸å¿ƒé‚è¼¯ï¼šæ¯æ¬¡è¿­ä»£éƒ½è¦åŸ·è¡Œï¼ˆ1222_vincent çš„æ–¹æ³•ï¼‰
                node = root
                
                # Selection: é¸æ“‡æœ€ä½³å­ç¯€é»ï¼Œç›´åˆ°æ‰¾åˆ°æœªå®Œå…¨å±•é–‹çš„ç¯€é»
                while node.is_fully_expanded() and node.children:
                    node = node.select_best_child()
                
                # Expansion: å¦‚æœç¯€é»æœªå®Œå…¨å±•é–‹ï¼Œå±•é–‹ä¸€å€‹æ–°çš„å­ç¯€é»
                if not node.is_fully_expanded():
                    unexpanded = [
                        n for n in node.taxonomy_data.get('children', {})
                        if n not in [c.name for c in node.children]
                    ]
                    if unexpanded:
                        child_name = np.random.choice(unexpanded)
                        child_node = MCTSNode(
                            child_name,
                            node,
                            node.taxonomy_data['children'][child_name]
                        )
                        node.children.append(child_node)
                        node = child_node
                
                # è¨ˆç®—èªç¾©çå‹µ
                semantic_reward = 0.0
                if node.taxonomy_data.get('embedding') is not None:
                    node_embedding = node.taxonomy_data.get('embedding')
                    if isinstance(node_embedding, list):
                        node_embedding = torch.tensor(node_embedding)
                    semantic_reward = util.cos_sim(
                        query_embedding,
                        node_embedding
                    ).item()
                
                # è¨ˆç®—é¡åˆ¥åŒ¹é…çå‹µ
                category_reward = self._calculate_category_reward(node, function_categories)
                
                # âœ… è¨ˆç®—é—œéµå­—åŒ¹é…çå‹µ
                keyword_reward = 0.0
                if extracted_keywords and node.taxonomy_data.get('is_leaf', False):
                    # ç²å–è©²ç¯€é»çš„ example_use_cases
                    path_nodes = []
                    curr = node
                    while curr is not None:
                        path_nodes.append(curr.name)
                        curr = curr.parent
                    path_str = " -> ".join(reversed(path_nodes))
                    
                    # âœ… ä¿®å¾©ï¼šnode_database ä¸­çš„ path_str ä¸åŒ…å« "Taxonomy ->" å‰ç¶´
                    search_path = path_str
                    if path_str.startswith("Taxonomy -> "):
                        search_path = path_str[len("Taxonomy -> "):]  # å»æ‰ "Taxonomy -> " å‰ç¶´
                    
                    db_node = next(
                        (item for item in self.node_database if item["path_str"] == search_path),
                        None
                    )
                    
                    if db_node:
                        use_cases = db_node.get("example_use_cases", [])
                        match_result = self._fuzzy_match_use_cases(extracted_keywords, use_cases)
                        keyword_reward = match_result["match_score"]
                
                # æ··åˆçå‹µï¼ˆèª¿æ•´æ¬Šé‡ï¼šsemantic*0.5 + categories*0.2 + keywords*0.3ï¼‰
                total_reward = 0.5 * semantic_reward + 0.2 * category_reward + 0.3 * keyword_reward
                
                # å›æº¯æ›´æ–°ï¼ˆ1222_vincent çš„æ–¹æ³•ï¼‰
                temp_node = node
                while temp_node is not None:
                    temp_node.visits += 1
                    temp_node.total_reward += total_reward
                    temp_node = temp_node.parent
        
        # æ”¶é›†çµæœï¼šå¾æ‰€æœ‰æ ¹ç¯€é»çš„è¨ªå•éçš„è‘‰å­ç¯€é»ä¸­æ”¶é›†
        leaf_nodes_visited = []
        
        def collect_visited_leafs(node: MCTSNode):
            """éæ­¸æ”¶é›†æ‰€æœ‰è¨ªå•éçš„è‘‰å­ç¯€é»ï¼ˆå¾å·²å±•é–‹çš„ç¯€é»ï¼‰"""
            if node.taxonomy_data.get('is_leaf', False):
                if node.visits > 0:  # åªæ”¶é›†è¨ªå•éçš„ç¯€é»
                    leaf_nodes_visited.append(node)
                return
            
            # éæ­¸è™•ç†å­ç¯€é»
            for child in node.children:
                collect_visited_leafs(child)
        
        # âœ… å¾å–®ä¸€æ ¹ç¯€é»æ”¶é›†ï¼ˆ1222_vincent çš„æ–¹æ³•ï¼‰
        collect_visited_leafs(root)
        
        # èª¿è©¦ä¿¡æ¯
        print(f"   - MCTS visited {root.visits} times")
        print(f"   - Found {len(leaf_nodes_visited)} visited leaf nodes")
        
        # å¦‚æœé‚„æ˜¯æ²’æœ‰æ‰¾åˆ°è‘‰å­ç¯€é»ï¼Œå˜—è©¦æ”¶é›†æ‰€æœ‰è¨ªå•éçš„ç¯€é»ï¼ˆä¸åƒ…åƒ…æ˜¯è‘‰å­ç¯€é»ï¼‰
        if not leaf_nodes_visited:
            print("   - No visited leaf nodes found, collecting all visited nodes...")
            all_visited_nodes = []
            
            def collect_all_visited(node: MCTSNode):
                """æ”¶é›†æ‰€æœ‰è¨ªå•éçš„ç¯€é»"""
                if node.visits > 0:
                    all_visited_nodes.append(node)
                for child in node.children:
                    collect_all_visited(child)
            
            collect_all_visited(root)
            print(f"   - Found {len(all_visited_nodes)} total visited nodes (including non-leaves)")
            
            # å¦‚æœæ‰¾åˆ°è¨ªå•éçš„ç¯€é»ï¼Œå˜—è©¦å¾ä¸­æ‰¾å‡ºæœ€æ¥è¿‘è‘‰å­ç¯€é»çš„ç¯€é»
            if all_visited_nodes:
                # æ‰¾å‡ºæ·±åº¦æœ€æ·±çš„ç¯€é»ï¼ˆæœ€æ¥è¿‘è‘‰å­ç¯€é»ï¼‰
                deepest_nodes = []
                max_depth = 0
                for n in all_visited_nodes:
                    depth = 0
                    curr = n
                    while curr.parent:
                        depth += 1
                        curr = curr.parent
                    if depth > max_depth:
                        max_depth = depth
                        deepest_nodes = [n]
                    elif depth == max_depth:
                        deepest_nodes.append(n)
                
                # å¾æœ€æ·±çš„ç¯€é»ä¸­ï¼Œæ‰¾å‡ºæœ‰ mapped_nodes çš„ï¼ˆå³ä½¿ä¸æ˜¯ is_leafï¼‰
                for n in deepest_nodes:
                    if n.taxonomy_data.get('mapped_nodes'):
                        # é€™å€‹ç¯€é»æœ‰ mapped_nodesï¼Œå³ä½¿ä¸æ˜¯ is_leafï¼Œä¹Ÿæ‡‰è©²è¢«è€ƒæ…®
                        leaf_nodes_visited.append(n)
                
                print(f"   - Found {len(leaf_nodes_visited)} nodes with mapped_nodes from visited nodes")
        
        # ç§»é™¤é—œéµå­—åŒ¹é…çš„ fallbackï¼ˆn8n ä¸éœ€è¦ï¼‰
        
        # æ’åºä¸¦è½‰æ›ç‚ºçµæœæ ¼å¼
        sorted_leafs = sorted(
            leaf_nodes_visited,
            key=lambda n: n.total_reward / (n.visits + 1e-6),
            reverse=True
        )
        
        print(f"   - Sorted {len(sorted_leafs)} visited leaf nodes by reward")
        
        all_leaf_results = []
        semantic_results = []  # ç”¨æ–¼æ”¶é›†èªç¾©åŒ¹é…çµæœ
        category_results = []  # ç”¨æ–¼æ”¶é›†é¡åˆ¥åŒ¹é…çµæœ
        
        for leaf in sorted_leafs:
            path_nodes = []
            curr = leaf
            while curr is not None:
                path_nodes.append(curr.name)
                curr = curr.parent
            path_str = " -> ".join(reversed(path_nodes))
            
            # âœ… ä¿®å¾©ï¼šnode_database ä¸­çš„ path_str ä¸åŒ…å« "Taxonomy ->" å‰ç¶´
            # æ‰€ä»¥éœ€è¦å»æ‰ "Taxonomy ->" å‰ç¶´ä¾†åŒ¹é…
            search_path = path_str
            if path_str.startswith("Taxonomy -> "):
                search_path = path_str[len("Taxonomy -> "):]  # å»æ‰ "Taxonomy -> " å‰ç¶´
            
            db_node = next(
                (item for item in self.node_database if item["path_str"] == search_path),
                None
            )
            
            if db_node:
                avg_reward = leaf.total_reward / (leaf.visits + 1e-6)
                
                # é‡æ–°è¨ˆç®— semantic å’Œ category åˆ†æ•¸ç”¨æ–¼èª¿è©¦
                semantic_score = 0.0
                category_score = 0.0
                
                # è¨ˆç®—èªç¾©åˆ†æ•¸
                # æª¢æŸ¥æ˜¯å¦æœ‰ embeddingï¼Œå¦‚æœæ²’æœ‰ï¼Œå¾ node_database ä¸­ç²å–
                leaf_embedding = leaf.taxonomy_data.get('embedding')
                if leaf_embedding is None:
                    # å˜—è©¦å¾ node_database ä¸­ç²å– embedding
                    db_node_embedding = db_node.get('embedding')
                    if db_node_embedding is not None:
                        leaf_embedding = db_node_embedding
                
                if leaf_embedding is not None:
                    # ç¢ºä¿ embedding æ˜¯ tensor
                    if isinstance(leaf_embedding, list):
                        leaf_embedding = torch.tensor(leaf_embedding)
                    semantic_score = util.cos_sim(
                        query_embedding,
                        leaf_embedding
                    ).item()
                    
                    # èª¿è©¦ï¼šé¡¯ç¤º semantic matching çš„è©³ç´°ä¿¡æ¯
                    description = db_node.get('description', '')
                    combined_text_used = db_node.get('combined_text', f"{path_str}: {description}")
                    print(f"   - Semantic Match Details for '{path_str[:60]}...':")
                    print(f"      Combined text used: '{combined_text_used[:100]}...'")
                    print(f"      Semantic score: {semantic_score:.4f}")
                    print(f"      Query: '{semantic_query[:60]}...'")
                else:
                    print(f"   - Warning: No embedding for {path_str}")
                    # èª¿è©¦ï¼šæ‰¾å‡ºç‚ºä»€éº¼æ²’æœ‰ embedding
                    description = db_node.get('description', '')
                    expected_combined_text = f"{path_str}: {description}"
                    print(f"      Expected combined_text: '{expected_combined_text[:80]}...'")
                    print(f"      Available in text_embedding_map: {expected_combined_text in self.text_embedding_map}")
                    # æª¢æŸ¥æ˜¯å¦æœ‰é¡ä¼¼çš„ combined_text
                    similar_texts = [text for text in self.text_embedding_map.keys() if path_str in text]
                    if similar_texts:
                        print(f"      Similar texts found: {len(similar_texts)}")
                        for st in similar_texts[:3]:
                            print(f"        - '{st[:80]}...'")
                
                # è¨ˆç®—é¡åˆ¥åˆ†æ•¸
                category_score = self._calculate_category_reward(leaf, function_categories)
                
                # âœ… è¨ˆç®—é—œéµå­—åŒ¹é…åˆ†æ•¸
                keyword_score = 0.0
                keyword_matches = []
                if extracted_keywords:
                    use_cases = db_node.get("example_use_cases", [])
                    match_result = self._fuzzy_match_use_cases(extracted_keywords, use_cases)
                    keyword_score = match_result["match_score"]
                    keyword_matches = match_result.get("matched_cases", [])
                
                # èª¿è©¦ï¼šè¼¸å‡ºé¡åˆ¥åŒ¹é…è©³æƒ…
                if category_score == 0.0 and function_categories:
                    path_nodes_for_cat = []
                    curr = leaf
                    while curr is not None:
                        path_nodes_for_cat.append(curr.name)
                        curr = curr.parent
                    full_path_str = " ".join(reversed(path_nodes_for_cat)).lower()
                    print(f"   - Debug category: path='{full_path_str[:50]}...', categories={function_categories}")
                
                # è¨˜éŒ„èªç¾©å’Œé¡åˆ¥åŒ¹é…çµæœ
                semantic_results.append({
                    'path_str': path_str,
                    'semantic_score': semantic_score,
                    'avg_reward': avg_reward
                })
                
                category_results.append({
                    'path_str': path_str,
                    'category_score': category_score,
                    'avg_reward': avg_reward
                })
                
                # âœ… å°æœ‰é—œéµå­—åŒ¹é…çš„ç¯€é»é™ä½é–¾å€¼ï¼ˆ1222_vincent çš„æ–¹æ³•ï¼‰
                has_keyword_match = keyword_score > 0
                threshold = 0.20 if has_keyword_match else 0.35
                
                if avg_reward > threshold:
                    # âœ… å°‡åŒ¹é…çµæœä¹Ÿå­˜å…¥ï¼ˆ1222_vincent çš„æ–¹æ³•ï¼‰
                    # âœ… ä¿®å¾©ï¼šåŒ…å«æ‰€æœ‰åˆ†æ•¸ï¼Œä»¥ä¾¿åœ¨ "Scoring Process" ä¸­æ­£ç¢ºé¡¯ç¤º
                    db_node_with_match = {
                        **db_node,
                        'avg_reward': avg_reward,
                        'semantic_score': semantic_score,
                        'category_score': category_score,
                        'keyword_score': keyword_score,
                        'visits': leaf.visits,
                        'total_reward': leaf.total_reward,
                        'keyword_matches': keyword_matches if has_keyword_match else []
                    }
                    all_leaf_results.append(db_node_with_match)
            else:
                print(f"   - Warning: Could not find db_node for path: {path_str}")
        
        # è¼¸å‡ºèªç¾©åŒ¹é…çµæœï¼ˆMCTS æ‰¾åˆ°çš„ç¯€é»ï¼‰
        print(f"\n   ğŸ“Š Semantic Matching Results (MCTS found {len(semantic_results)} nodes):")
        semantic_sorted = sorted(semantic_results, key=lambda x: x['semantic_score'], reverse=True)
        for i, result in enumerate(semantic_sorted):
            path_preview = result['path_str'][:70] + "..." if len(result['path_str']) > 70 else result['path_str']
            print(f"      {i+1}. {path_preview}")
            print(f"         Semantic Score: {result['semantic_score']:.4f} | Avg Reward: {result['avg_reward']:.4f}")
            
            # é¡¯ç¤ºè©²ç¯€é»ä½¿ç”¨çš„ combined_textï¼ˆç”¨æ–¼èª¿è©¦ embedding è³ªé‡ï¼‰
            db_node = next(
                (item for item in self.node_database if item["path_str"] == result['path_str']),
                None
            )
            if db_node:
                combined_text = db_node.get('combined_text', 'N/A')
                description = db_node.get('description', 'N/A')
                print(f"         Combined text: '{combined_text[:90]}...'")
                print(f"         Description: '{description[:60]}...'")
                print(f"         Query: '{semantic_query[:60]}...'")
                print(f"         ---")
        
        # è¼¸å‡ºé¡åˆ¥åŒ¹é…çµæœï¼ˆGPT æå–çš„ function categoriesï¼‰
        print(f"\n   ğŸ“Š Category Matching Results (GPT categories: {function_categories}):")
        category_sorted = sorted(category_results, key=lambda x: x['category_score'], reverse=True)
        for i, result in enumerate(category_sorted):
            path_preview = result['path_str'][:70] + "..." if len(result['path_str']) > 70 else result['path_str']
            print(f"      {i+1}. {path_preview}")
            print(f"         Category Score: {result['category_score']:.4f} | Avg Reward: {result['avg_reward']:.4f}")
            
            # é¡¯ç¤ºåŒ¹é…çš„ categories
            path_lower = result['path_str'].lower()
            matched_cats = [cat for cat in function_categories if cat.lower() in path_lower]
            if matched_cats:
                print(f"         Matched Categories: {matched_cats}")
            else:
                print(f"         Matched Categories: None (path doesn't contain any GPT categories)")
        
        # è¼¸å‡ºç®—åˆ†éç¨‹
        print(f"\n   ğŸ“Š Scoring Process (for each node):")
        for i, result in enumerate(all_leaf_results[:5]):  # é¡¯ç¤ºå‰5å€‹çš„è©³ç´°ç®—åˆ†
            path_preview = result['path_str'][:60] + "..." if len(result['path_str']) > 60 else result['path_str']
            semantic = result.get('semantic_score', 0.0)
            category = result.get('category_score', 0.0)
            avg_reward = result.get('avg_reward', 0.0)
            visits = result.get('visits', 0)
            total_reward = result.get('total_reward', 0.0)
            
            # è¨ˆç®—å…¬å¼ï¼štotal_reward = 0.5 * semantic + 0.2 * category + 0.3 * keyword
            keyword = result.get('keyword_score', 0.0)
            calculated_reward = 0.5 * semantic + 0.2 * category + 0.3 * keyword
            
            print(f"      Node {i+1}: {path_preview}")
            print(f"         - Semantic Score: {semantic:.4f} (weight: 0.5)")
            print(f"         - Category Score: {category:.4f} (weight: 0.2)")
            print(f"         - Keyword Score: {keyword:.4f} (weight: 0.3)")
            print(f"         - Calculated Reward: {calculated_reward:.4f} = 0.5 * {semantic:.4f} + 0.2 * {category:.4f} + 0.3 * {keyword:.4f}")
            print(f"         - MCTS Visits: {visits} | Total Reward: {total_reward:.4f} | Avg Reward: {avg_reward:.4f}")
        
        # æŒ‰å¹³å‡çå‹µæ’åº
        all_leaf_results.sort(key=lambda x: x.get('avg_reward', 0), reverse=True)
        
        print(f"   - Collected {len(all_leaf_results)} results from visited leaf nodes")
        if all_leaf_results:
            print(f"   - Top result: {all_leaf_results[0].get('path_str', 'N/A')} (reward: {all_leaf_results[0].get('avg_reward', 0):.3f})")
        
        # âœ… ä½¿ç”¨ 1222_vincent çš„æ–¹æ³•ï¼šåªç¢ºä¿é—œéµé¡åˆ¥è¦†è“‹ï¼Œä¸ä¸»å‹•æ“´å±•æ¯å€‹å¤§é¡
        results = self._ensure_category_coverage(all_leaf_results, function_categories, top_n)
        
        return results
    
    def _get_path_to_root(self, node: MCTSNode) -> List[MCTSNode]:
        """ç²å–å¾ç¯€é»åˆ°æ ¹çš„è·¯å¾‘"""
        path = []
        curr = node
        while curr is not None:
            path.append(curr)
            curr = curr.parent
        return path[::-1]
    
    
    def _select_results_by_category_coverage(
        self, 
        all_results: List[Dict], 
        function_categories: List[str], 
        top_n: int,
        query_embedding=None,
        extracted_keywords: Optional[Set[str]] = None
    ) -> List[Dict]:
        """
        æŒ‰ç¬¬ä¸€å±¤åˆ†é¡åˆ†çµ„é¸æ“‡çµæœï¼Œç¢ºä¿æ¯å€‹å¤§é¡å¯ä»¥è¿”å›å¤šå€‹å­è‘‰
        
        å³ä½¿ MCTS åªæ‰¾åˆ°ä¸€å€‹ç¯€é»ï¼Œä¹Ÿè¦å¾è©²å¤§é¡ä¸­æ“´å±•é¸æ“‡å¤šå€‹ç›¸é—œçš„å­è‘‰ã€‚
        ä¾‹å¦‚ï¼šå°æ–¼ "AI, ML & Automation Intelligence" å¤§é¡ï¼Œå¯ä»¥è¿”å›ï¼š
        - 6.1.1 LLM Providers
        - 6.2.1 Memory Stores
        - 6.4.1 Tool Augmentation
        ç­‰å¤šå€‹ç›¸é—œå­è‘‰
        """
        if not all_results:
            return []
        
        # æå–ç¬¬ä¸€å±¤åˆ†é¡ï¼ˆä¾‹å¦‚ï¼š"6 AI, ML & Automation Intelligence"ï¼‰
        def get_first_level_category(path_str: str) -> str:
            """å¾è·¯å¾‘ä¸­æå–ç¬¬ä¸€å±¤åˆ†é¡"""
            parts = path_str.split(" -> ")
            if parts:
                first_part = parts[0]
                # å»æ‰æ•¸å­—å‰ç¶´ï¼ˆä¾‹å¦‚ï¼š"6 AI, ML & Automation Intelligence" -> "AI, ML & Automation Intelligence"ï¼‰
                first_parts = first_part.split(' ', 1)
                if len(first_parts) == 2 and first_parts[0].isdigit():
                    return first_parts[1]
                return first_part
            return "Unknown"
        
        # æŒ‰ç¬¬ä¸€å±¤åˆ†é¡åˆ†çµ„ MCTS çµæœ
        category_groups = {}
        for result in all_results:
            path_str = result.get('path_str', '')
            category = get_first_level_category(path_str)
            if category not in category_groups:
                category_groups[category] = []
            category_groups[category].append(result)
        
        print(f"   - Results grouped into {len(category_groups)} top-level categories:")
        for cat, group_results in category_groups.items():
            print(f"      - {cat}: {len(group_results)} nodes from MCTS")
        
        # âœ… é—œéµæ”¹é€²ï¼šå°æ–¼æ¯å€‹ GPT categoryï¼Œå¾ node_database ä¸­æ“´å±•é¸æ“‡åŒä¸€å¤§é¡çš„å…¶ä»–ç¯€é»
        selected_results = []
        selected_paths = set()
        
        # å…ˆæ·»åŠ  MCTS æ‰¾åˆ°çš„çµæœ
        for result in all_results:
            path_str = result.get('path_str', '')
            selected_results.append(result)
            selected_paths.add(path_str)
        
        # âœ… æ”¹é€²ï¼šéœ€è¦ query_embedding å’Œ extracted_keywords ä¾†è¨ˆç®—æ“´å±•ç¯€é»çš„åˆ†æ•¸
        # é€™äº›åƒæ•¸éœ€è¦åœ¨æ–¹æ³•ç°½åä¸­å‚³å…¥ï¼Œæˆ–è€…ä½œç‚ºé¡çš„å±¬æ€§
        # æš«æ™‚å¾å¤–éƒ¨ç²å–ï¼ˆé€šé closure æˆ–åƒæ•¸å‚³éï¼‰
        # æ³¨æ„ï¼šé€™è£¡éœ€è¦ query_embeddingï¼Œä½†å®ƒåœ¨ search_with_categories æ–¹æ³•ä¸­
        # æˆ‘å€‘éœ€è¦å°‡å®ƒä½œç‚ºåƒæ•¸å‚³å…¥ï¼Œæˆ–è€…å­˜å„²ç‚ºå¯¦ä¾‹è®Šé‡
        
        # å°æ–¼æ¯å€‹ GPT categoryï¼Œæ“´å±•é¸æ“‡åŒä¸€å¤§é¡çš„å…¶ä»–ç¯€é»
        for gpt_category in function_categories:
            gpt_cat_lower = gpt_category.lower()
            
            # æ‰¾åˆ°åŒ¹é…çš„ç¬¬ä¸€å±¤åˆ†é¡ï¼ˆå¾ MCTS çµæœä¸­ï¼‰
            matching_categories = []
            for cat, group_results in category_groups.items():
                if gpt_cat_lower in cat.lower() or cat.lower() in gpt_cat_lower:
                    matching_categories.append(cat)
            
            # å°æ–¼æ¯å€‹åŒ¹é…çš„åˆ†é¡ï¼Œå¾ node_database ä¸­é¸æ“‡æ›´å¤šç¯€é»
            for matching_cat in matching_categories:
                # âœ… å„ªåŒ–ï¼šé å…ˆéæ¿¾ï¼Œåªè™•ç†å±¬æ–¼è©²å¤§é¡çš„ç¯€é»ï¼Œé¿å…é‡è¤‡éæ­·
                # å…ˆå¿«é€Ÿéæ¿¾å‡ºå±¬æ–¼è©²å¤§é¡çš„ç¯€é»
                category_nodes = [
                    db_node for db_node in self.node_database
                    if get_first_level_category(db_node.get('path_str', '')) == matching_cat
                    and db_node.get('path_str', '') not in selected_paths
                ]
                
                if not category_nodes:
                    continue
                
                # âœ… å„ªåŒ–ï¼šé™åˆ¶å€™é¸ç¯€é»æ•¸é‡ï¼Œåªè¨ˆç®—å‰ 50 å€‹çš„èªç¾©ç›¸ä¼¼åº¦ï¼ˆé¿å…è¨ˆç®—å¤ªå¤šï¼‰
                # å…ˆæŒ‰ mapped_nodes æ•¸é‡æ’åºï¼Œå„ªå…ˆè€ƒæ…®æœ‰æ›´å¤šç¯€é»çš„ taxonomy ç¯€é»
                category_nodes.sort(key=lambda x: len(x.get('mapped_nodes', [])), reverse=True)
                category_nodes = category_nodes[:50]  # åªè™•ç†å‰ 50 å€‹
                
                candidates_from_db = []
                for db_node in category_nodes:
                    db_path_str = db_node.get('path_str', '')
                    
                    # âœ… æ”¹é€²ï¼šè¨ˆç®—èªç¾©ç›¸ä¼¼åº¦ï¼Œé¸æ“‡æ›´ç›¸é—œçš„ç¯€é»
                    # ç²å–è©²ç¯€é»çš„ combined_text å’Œ embedding
                    combined_text = db_node.get('combined_text', f"{db_path_str}: {db_node.get('description', '')}")
                    node_embedding = self.text_embedding_map.get(combined_text)
                    
                    semantic_score = 0.0
                    if node_embedding is not None:
                        if isinstance(node_embedding, list):
                            node_embedding = torch.tensor(node_embedding)
                        # è¨ˆç®—èˆ‡æŸ¥è©¢çš„èªç¾©ç›¸ä¼¼åº¦
                        semantic_score = util.cos_sim(query_embedding, node_embedding).item()
                    
                    # âœ… å„ªåŒ–ï¼šåªåœ¨èªç¾©åˆ†æ•¸è¼ƒé«˜æ™‚æ‰è¨ˆç®—é—œéµå­—åŒ¹é…ï¼ˆé¿å…ä¸å¿…è¦çš„è¨ˆç®—ï¼‰
                    keyword_score = 0.0
                    if extracted_keywords and semantic_score > 0.15:  # åªå°èªç¾©ç›¸é—œçš„ç¯€é»è¨ˆç®—é—œéµå­—
                        use_cases = db_node.get("example_use_cases", [])
                        if use_cases:  # åªåœ¨æœ‰ use_cases æ™‚æ‰è¨ˆç®—
                            match_result = self._fuzzy_match_use_cases(extracted_keywords, use_cases, semantic_threshold=0.25)
                            keyword_score = match_result["match_score"]
                    
                    # è¨ˆç®—ç¶œåˆåˆ†æ•¸ï¼šsemantic*0.5 + category*0.2 + keyword*0.3
                    category_score = 0.5  # å› ç‚ºåŒ¹é…äº† category
                    calculated_reward = 0.5 * semantic_score + 0.2 * category_score + 0.3 * keyword_score
                    
                    candidates_from_db.append({
                        **db_node,
                        'avg_reward': calculated_reward,
                        'semantic_score': semantic_score,
                        'category_score': category_score,
                        'keyword_score': keyword_score,
                        'visits': 0,
                        'total_reward': calculated_reward
                    })
                
                # æŒ‰ reward æ’åºï¼ˆç¾åœ¨æœ‰çœŸå¯¦çš„åˆ†æ•¸äº†ï¼‰
                candidates_from_db.sort(key=lambda x: x.get('avg_reward', 0), reverse=True)
                
                # å°æ–¼æ¯å€‹åŒ¹é…çš„å¤§é¡ï¼Œé¸æ“‡æœ€å¤š 3 å€‹ç¯€é»ï¼ˆåŒ…æ‹¬ MCTS æ‰¾åˆ°çš„ï¼‰
                # å¦‚æœ MCTS å·²ç¶“æ‰¾åˆ°äº†ä¸€äº›ï¼Œå†è£œå……åˆ° 3 å€‹
                mcts_count = len([r for r in all_results if get_first_level_category(r.get('path_str', '')) == matching_cat])
                max_per_category = 3
                needed = max(0, max_per_category - mcts_count)
                
                print(f"   - Expanding '{matching_cat}': MCTS found {mcts_count} nodes, adding {needed} more from database")
                print(f"      Found {len(candidates_from_db)} candidates, selecting top {needed} by semantic+category+keyword score")
                
                for candidate in candidates_from_db[:needed]:
                    path_str = candidate.get('path_str', '')
                    if path_str not in selected_paths:
                        selected_results.append(candidate)
                        selected_paths.add(path_str)
                        print(f"      Added: {path_str[:60]}... (reward: {candidate.get('avg_reward', 0):.3f}, semantic: {candidate.get('semantic_score', 0):.3f})")
                        if len(selected_results) >= top_n * 2:  # å…è¨±æ›´å¤šçµæœï¼Œå¾ŒçºŒæœƒé™åˆ¶
                            break
                if len(selected_results) >= top_n * 2:
                    break
        
        # æŒ‰ reward æ’åº
        selected_results.sort(key=lambda x: x.get('avg_reward', 0), reverse=True)
        
        # é™åˆ¶æœ€çµ‚æ•¸é‡
        final_results = selected_results[:top_n] if len(selected_results) > top_n else selected_results
        
        print(f"   - Selected {len(final_results)} results (expanded from {len(all_results)} MCTS results)")
        return final_results
    
    def _has_category_match(self, db_node: Dict, categories: List[str]) -> bool:
        """æª¢æŸ¥ç¯€é»æ˜¯å¦åŒ¹é…é¡åˆ¥"""
        if not categories:
            return False
        
        path_text = f"{db_node.get('path_str', '')} {db_node.get('description', '')}".lower()
        if any(category.lower() in path_text for category in categories):
            return True
        
        mapped_nodes_text = " ".join(db_node.get('mapped_nodes', [])).lower()
        if any(category.lower() in mapped_nodes_text for category in categories):
            return True
        
        return False
    
    def _ensure_category_coverage(self, results: List[Dict], categories: List[str], top_n: int) -> List[Dict]:
        """ç¢ºä¿é¡åˆ¥è¦†è“‹"""
        critical_categories = ['SMS', 'Email', 'Payment', 'Database', 'API']
        
        for category in critical_categories:
            if category not in categories:
                continue
            
            has_coverage = any(self._has_category_match(r, [category]) for r in results)
            
            if not has_coverage:
                for db_node in self.node_database:
                    if self._has_category_match(db_node, [category]):
                        if not any(r['path_str'] == db_node['path_str'] for r in results):
                            db_node_with_reward = {**db_node, 'avg_reward': 0.40}
                            results.append(db_node_with_reward)
                            print(f" - Category boost: Forcibly added '{db_node['path_str']}' for category '{category}'")
                        break
        
        # é‡æ–°æ’åºä¸¦é™åˆ¶æœ€çµ‚æ•¸é‡
        results.sort(key=lambda x: x.get('avg_reward', 0), reverse=True)
        return results[:top_n]
    
    def search_by_keywords(self, keywords: Set[str]) -> List[Dict]:
        """
        åŸ·è¡Œç›´æ¥çš„é—œéµå­—æƒæï¼ˆLexical Searchï¼‰
        
        Args:
            keywords: é—œéµå­—é›†åˆ
        
        Returns:
            keyword_hits: åŒ¹é…çš„ç¯€é»åˆ—è¡¨
        """
        if not keywords:
            return []
        
        print(" - Performing direct keyword scan...")
        keyword_hits = []
        lower_keywords = {kw.lower() for kw in keywords}
        
        for db_node in self.node_database:
            # æª¢æŸ¥è·¯å¾‘å’Œæè¿°
            path_str = db_node.get('path_str', '').lower()
            description = db_node.get('description', '').lower()
            mapped_nodes = " ".join(db_node.get('mapped_nodes', [])).lower()
            
            # çµ„åˆæ‰€æœ‰æ–‡æœ¬
            text_to_check = f"{path_str} {description} {mapped_nodes}"
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•é—œéµå­—åŒ¹é…ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
            found_in_text = False
            matched_keywords = []
            for kw in lower_keywords:
                # æª¢æŸ¥é—œéµå­—æ˜¯å¦åœ¨æ–‡æœ¬ä¸­
                if kw in text_to_check:
                    found_in_text = True
                    matched_keywords.append(kw)
                # ä¹Ÿæª¢æŸ¥é—œéµå­—æ˜¯å¦åœ¨ç¯€é»é¡å‹ä¸­ï¼ˆå¦‚ "ocr" åœ¨ "n8n-nodes-base.ocr"ï¼‰
                elif any(kw in node.lower() for node in db_node.get('mapped_nodes', [])):
                    found_in_text = True
                    matched_keywords.append(kw)
            
            # ä¹Ÿæª¢æŸ¥ example_use_casesï¼ˆå¦‚æœæœ‰çš„è©±ï¼ŒåƒåŸæœ¬çš„ç¨‹å¼ç¢¼ï¼‰
            use_cases = db_node.get("example_use_cases", [])
            match_result = self._fuzzy_match_use_cases(keywords, use_cases)
            found_in_cases = match_result["match_score"] > 0
            matched_use_cases = match_result.get("matched_cases", [])
            
            if found_in_text or found_in_cases:
                # æ ¹æ“šåŒ¹é…é¡å‹çµ¦ä¸åŒçš„åˆ†æ•¸
                if found_in_cases:
                    # å¦‚æœåŒ¹é…åˆ° use casesï¼Œçµ¦æ›´é«˜çš„åˆ†æ•¸ï¼ˆå› ç‚ºæ›´ç²¾ç¢ºï¼‰
                    reward = 0.6 + (match_result["match_score"] * 0.2)  # 0.6-0.8
                    match_reason = f"Keyword in use cases (score: {match_result['match_score']:.2f})"
                else:
                    # å¦‚æœåªåœ¨æ–‡æœ¬ä¸­åŒ¹é…ï¼Œçµ¦åŸºç¤åˆ†æ•¸
                    reward = 0.5
                    match_reason = "Keyword in path/description/mapped_nodes"
                
                db_node_with_match = {
                    **db_node,
                    'avg_reward': reward,
                    'keyword_matches': matched_keywords if matched_keywords else list(keywords),
                    'matched_use_cases': matched_use_cases,
                    'keyword_match_score': match_result["match_score"] if found_in_cases else 0.0,
                    'match_reason': match_reason
                }
                keyword_hits.append(db_node_with_match)
        
        print(f" - Direct keyword scan found {len(keyword_hits)} hits.")
        return keyword_hits
    
    def _fuzzy_match_use_cases(self, keywords: Set[str], use_cases: List[str], semantic_threshold: float = 0.3) -> Dict:
        """
        å° example_use_cases é€²è¡Œèªç¾©åŒ¹é…ï¼ˆä½¿ç”¨ embedding ç›¸ä¼¼åº¦ï¼‰
        
        ä½¿ç”¨èªç¾©åŒ¹é…è€Œä¸æ˜¯ç´”æ–‡å­—åŒ¹é…ï¼Œå› ç‚º GPT æå–çš„ keywords å’Œ example_use_cases
        ä¹‹é–“å¯èƒ½æ²’æœ‰ç›´æ¥çš„æ–‡å­—åŒ¹é…ï¼Œä½†èªç¾©ä¸Šå¯èƒ½ç›¸é—œã€‚
        
        Args:
            keywords: GPT æå–çš„é—œéµå­—é›†åˆ
            use_cases: example_use_cases åˆ—è¡¨
            semantic_threshold: èªç¾©ç›¸ä¼¼åº¦é–¾å€¼ï¼ˆ0-1ï¼‰ï¼Œä½æ–¼æ­¤å€¼ä¸èªç‚ºåŒ¹é…
        
        Returns: {"matched_cases": [...], "match_score": 0.0-1.0, "semantic_scores": {...}}
        """
        if not use_cases or not keywords:
            return {"matched_cases": [], "match_score": 0.0, "semantic_scores": {}}
        
        # çµ„åˆæ‰€æœ‰ keywords ç‚ºä¸€å€‹æŸ¥è©¢æ–‡æœ¬
        query_text = " ".join(keywords)
        
        # âœ… å„ªåŒ–ï¼šç·©å­˜æŸ¥è©¢ embeddingï¼ˆå¦‚æœ keywords ç›¸åŒï¼Œå¯ä»¥é‡ç”¨ï¼‰
        query_cache_key = query_text
        if query_cache_key not in self.use_case_embedding_cache:
            query_embedding = self.model.encode(query_text, convert_to_tensor=True)
            self.use_case_embedding_cache[query_cache_key] = query_embedding
        else:
            query_embedding = self.use_case_embedding_cache[query_cache_key]
        
        # âœ… å„ªåŒ–ï¼šç·©å­˜ use_case embeddingsï¼Œé¿å…é‡è¤‡è¨ˆç®—
        use_case_embeddings_list = []
        for use_case in use_cases:
            if use_case in self.use_case_embedding_cache:
                use_case_embeddings_list.append(self.use_case_embedding_cache[use_case])
            else:
                embedding = self.model.encode(use_case, convert_to_tensor=True)
                self.use_case_embedding_cache[use_case] = embedding
                use_case_embeddings_list.append(embedding)
        
        # å°‡åˆ—è¡¨è½‰æ›ç‚º tensorï¼ˆstackï¼‰
        use_case_embeddings = torch.stack(use_case_embeddings_list)
        
        # è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦ï¼ˆé¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰
        similarities = util.cos_sim(query_embedding, use_case_embeddings)[0]  # [0] å› ç‚º query åªæœ‰ä¸€å€‹
        
        # æ‰¾åˆ°åŒ¹é…çš„ use casesï¼ˆç›¸ä¼¼åº¦ >= thresholdï¼‰
        matched_cases = []
        matched_scores = {}
        semantic_scores = {}
        
        for i, (case, similarity) in enumerate(zip(use_cases, similarities)):
            similarity_value = similarity.item()
            semantic_scores[case] = similarity_value
            
            if similarity_value >= semantic_threshold:
                matched_cases.append(case)
                matched_scores[case] = similarity_value
        
        # è¨ˆç®—åŒ¹é…åˆ†æ•¸ï¼šä½¿ç”¨æœ€é«˜ç›¸ä¼¼åº¦æˆ–å¹³å‡ç›¸ä¼¼åº¦
        if matched_scores:
            # ä½¿ç”¨æœ€é«˜ç›¸ä¼¼åº¦ä½œç‚º match_score
            max_similarity = max(matched_scores.values())
            # æˆ–è€…ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦
            avg_similarity = sum(matched_scores.values()) / len(matched_scores) if matched_scores else 0.0
            # ç¶œåˆè€ƒæ…®ï¼šæœ€é«˜ç›¸ä¼¼åº¦ * 0.7 + å¹³å‡ç›¸ä¼¼åº¦ * 0.3
            match_score = max_similarity * 0.7 + avg_similarity * 0.3
        else:
            match_score = 0.0
        
        # èª¿è©¦ï¼šå¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œè¼¸å‡ºä¿¡æ¯
        if match_score > 0:
            print(f"   - Semantic keyword match: {len(matched_cases)}/{len(use_cases)} use cases matched (max similarity: {max(matched_scores.values()):.3f})")
        
        return {
            "matched_cases": matched_cases,
            "match_score": match_score,
            "semantic_scores": semantic_scores,
            "matched_scores": matched_scores
        }

