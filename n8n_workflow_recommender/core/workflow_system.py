#!/usr/bin/env python3
"""
æ ¸å¿ƒå·¥ä½œæµç¨‹ç³»çµ±

æ•´åˆ MCTS æœç´¢ã€A* è·¯å¾‘ç”Ÿæˆã€NLU ç­‰çµ„ä»¶ï¼Œç”Ÿæˆå€™é¸å·¥ä½œæµç¨‹ã€‚
"""

import json
from typing import List, Dict, Set, Optional
from pathlib import Path

from ..search.mcts_search_agent import TaxonomySearchAgent, MCTSNode
from ..generation.workflow_composer import DomainKnowledgeGraph, ModuleAwareWorkflowComposer
from ..nlu.intent_analyzer import IntentAnalyzer
from ..nlu.keyword_extractor import KeywordExtractor


class HybridWorkflowSystem:
    """
    æ··åˆå·¥ä½œæµç¨‹ç³»çµ±
    
    æ•´åˆæ‰€æœ‰çµ„ä»¶ï¼Œå¾ç”¨æˆ¶æŸ¥è©¢ç”Ÿæˆå€™é¸å·¥ä½œæµç¨‹ã€‚
    """
    
    def __init__(
        self,
        triples: List[tuple],
        ontology: Dict,
        taxonomy_path: str,
        openai_api_key: str
    ):
        """
        åˆå§‹åŒ–ç³»çµ±
        
        Args:
            triples: çŸ¥è­˜åœ–ä¸‰å…ƒçµ„åˆ—è¡¨
            ontology: Ontology å­—å…¸
            taxonomy_path: MCTS taxonomy æª”æ¡ˆè·¯å¾‘
            openai_api_key: OpenAI API å¯†é‘°
        """
        print("\n=== Initializing Hybrid Workflow System ===")
        
        # åˆå§‹åŒ–çµ„ä»¶
        print("1. Initializing Taxonomy Search Agent (MCTS)...")
        self.search_agent = TaxonomySearchAgent(taxonomy_path)
        
        print("2. Initializing Domain Knowledge Graph (A*)...")
        aux_keywords = ['é€šçŸ¥', 'ç™¼é€', 'Email', 'SMS', 'è¨˜éŒ„', 'æ—¥èªŒ', 'æé†’', 'ç¢ºèª']
        self.domain_graph = DomainKnowledgeGraph(triples, ontology, auxiliary_keywords=aux_keywords)
        
        print("3. Initializing Workflow Composer...")
        self.composer = ModuleAwareWorkflowComposer(
            self.domain_graph,
            self.search_agent,
            ontology
        )
        
        print("4. Building Function Categories from Taxonomy...")
        # å¾ taxonomy å‹•æ…‹æ§‹å»º function_categoriesï¼ˆåƒåŸæœ¬çš„ç¨‹å¼ç¢¼ï¼‰
        self.function_categories = self._build_categories_from_taxonomy(taxonomy_path)
        print(f"   - Loaded {len(self.function_categories)} function categories.")
        
        print("5. Initializing NLU Components...")
        # å‚³å…¥ ontology å’Œ function_categories ä»¥æä¾›æ›´å¥½çš„ä¸Šä¸‹æ–‡
        self.intent_analyzer = IntentAnalyzer(
            openai_api_key,
            ontology=ontology,
            function_categories=self.function_categories
        )
        self.keyword_extractor = KeywordExtractor()
        
        self.ontology = ontology
        
        print("âœ… All components initialized successfully.")
    
    def generate_workflow(self, user_query: str) -> List[Dict]:
        """
        ç”Ÿæˆå·¥ä½œæµç¨‹å€™é¸
        
        Args:
            user_query: ç”¨æˆ¶æŸ¥è©¢å­—ç¬¦ä¸²
        
        Returns:
            candidates: å€™é¸å·¥ä½œæµç¨‹åˆ—è¡¨
        """
        print("\n" + "=" * 80)
        print(f"Received User Query: '{user_query}'")
        print("=" * 80)
        
        # STAGE 0: NLU Analysis
        print("\nSTAGE 0: NLU Analysis")
        analysis = self.intent_analyzer.analyze(user_query)
        goal_description = analysis.get('goal_description', user_query)
        extracted_params = analysis.get('parameters', {})
        function_categories = analysis.get('function_categories', [])
        
        # æå–é—œéµå­—ï¼ˆä½¿ç”¨ LLMï¼ŒåƒåŸæœ¬çš„ç¨‹å¼ç¢¼ï¼‰
        keywords = self.intent_analyzer.extract_keywords(user_query, analysis)
        # ä¹Ÿæ·»åŠ ä¸€äº›æŠ€è¡“è¡“èªä½œç‚ºè£œå……
        tech_terms = self.keyword_extractor.extract_technical_terms(user_query)
        keywords.update(tech_terms)
        
        print(f" - Extracted Keywords: {keywords}")
        
        # STAGE 1: MCTS Search
        print("\nSTAGE 1: MCTS Taxonomy Search")
        # âœ… æ ¹æ“š taxonomy çµ±è¨ˆï¼š92 å€‹è‘‰å­ç¯€é»ï¼Œå»ºè­°è¿­ä»£æ¬¡æ•¸ç‚º 92 * 3 = 276
        # è¨­å®šç‚º 300 æ¬¡ï¼Œç¢ºä¿æœ‰è¶³å¤ çš„æ¢ç´¢ç©ºé–“ï¼ŒåŒæ™‚ä¸æœƒéåº¦æµªè²»æ™‚é–“
        matched_leaves = self.search_agent.search_with_categories(
            semantic_query=goal_description,
            function_categories=function_categories,
            extracted_keywords=keywords,
            iterations=300,  # å„ªåŒ–ï¼š92 å€‹è‘‰å­ç¯€é»ï¼Œ300 æ¬¡è¿­ä»£è¶³å¤ ï¼ˆç´„ 3.3x è¦†è“‹ç‡ï¼‰
            top_n=5
        )
        
        if not matched_leaves:
            print(" - No matches found in taxonomy. Trying keyword search...")
            matched_leaves = self.search_agent.search_by_keywords(keywords)
        
        if not matched_leaves:
            print("âš ï¸  Warning: No taxonomy matches found.")
            # å˜—è©¦ä½¿ç”¨æ›´å¯¬é¬†çš„æœç´¢ï¼šç›´æ¥å¾çŸ¥è­˜åœ–ä¸­æŸ¥æ‰¾ç›¸é—œç¯€é»
            print(" - Trying fallback: searching knowledge graph directly...")
            # å¾é—œéµå­—ä¸­æå–å¯èƒ½çš„ç¯€é»é¡å‹
            fallback_nodes = []
            for kw in keywords:
                # å˜—è©¦åœ¨ ontology ä¸­æŸ¥æ‰¾åŒ…å«é—œéµå­—çš„ç¯€é»
                for node_type in self.ontology.keys():
                    if kw.lower() in node_type.lower():
                        fallback_nodes.append(node_type)
                        if len(fallback_nodes) >= 5:
                            break
                if len(fallback_nodes) >= 5:
                    break
            
            if fallback_nodes:
                print(f" - Found {len(fallback_nodes)} fallback nodes from ontology")
                # å‰µå»ºä¸€å€‹ç°¡å–®çš„å€™é¸ï¼Œä½¿ç”¨ fallback ç¯€é»
                initial_concrete_nodes = list(set(fallback_nodes))
                # å‰µå»ºä¸€å€‹ç°¡å–®çš„å€™é¸å·¥ä½œæµç¨‹
                print("\nSTAGE 2: Workflow Composition (A*) - Fallback Mode")
                candidates = self.composer.compose(
                    matched_leaves=[],  # ç©ºçš„ matched_leaves
                    initial_concrete_nodes=initial_concrete_nodes,
                    user_query=user_query,
                    params=extracted_params
                )
                if candidates:
                    print(f" - Generated {len(candidates)} workflow candidates (fallback mode)")
                    return candidates
            else:
                print("âš ï¸  No fallback nodes found. Trying to create minimal workflow...")
                # æœ€å¾Œçš„å‚™ç”¨æ–¹æ¡ˆï¼šå‰µå»ºä¸€å€‹æœ€å°çš„å·¥ä½œæµç¨‹
                # ä½¿ç”¨å¸¸è¦‹çš„è§¸ç™¼ç¯€é»å’Œè™•ç†ç¯€é»
                minimal_nodes = []
                for node_type in ['n8n-nodes-base.manualTrigger', 'n8n-nodes-base.set', 'n8n-nodes-base.noOp']:
                    if node_type in self.ontology:
                        minimal_nodes.append(node_type)
                
                if minimal_nodes:
                    print(f" - Creating minimal workflow with {len(minimal_nodes)} nodes")
                    candidates = self.composer.compose(
                        matched_leaves=[],
                        initial_concrete_nodes=minimal_nodes,
                        user_query=user_query,
                        params=extracted_params
                    )
                    if candidates:
                        return candidates
            
            print("âš ï¸  All fallback strategies failed. Returning empty candidates.")
            return []
        
        print(f"\n   âœ… Final Selected Taxonomy Nodes ({len(matched_leaves)} nodes):")
        for i, leaf in enumerate(matched_leaves):
            path_str = leaf.get('path_str', 'N/A')
            semantic = leaf.get('semantic_score', 0.0)
            category = leaf.get('category_score', 0.0)
            avg_reward = leaf.get('avg_reward', 0.0)
            mapped_nodes = leaf.get('mapped_nodes', [])
            print(f"      {i+1}. {path_str}")
            print(f"         - Semantic: {semantic:.4f} | Category: {category:.4f} | Reward: {avg_reward:.4f}")
            print(f"         - Mapped Nodes: {len(mapped_nodes)} nodes")
        
        print(f"\n   ğŸ“¦ Extracting Concrete Node Types from Selected Taxonomy Nodes...")
        # æå–æ‰€æœ‰ mapped_nodes
        initial_concrete_nodes = []
        node_source_map = {}  # è¨˜éŒ„æ¯å€‹ç¯€é»ä¾†è‡ªå“ªå€‹ taxonomy node
        
        for leaf in matched_leaves:
            mapped_nodes = leaf.get('mapped_nodes', [])
            path_str = leaf.get('path_str', 'N/A')
            if mapped_nodes:
                initial_concrete_nodes.extend(mapped_nodes)
                # è¨˜éŒ„ä¾†æº
                for node in mapped_nodes:
                    if node not in node_source_map:
                        node_source_map[node] = []
                    node_source_map[node].append(path_str)
        
        # å»é‡ä¸¦ä¿æŒé †åº
        unique_nodes = []
        seen = set()
        for node in initial_concrete_nodes:
            if node not in seen:
                unique_nodes.append(node)
                seen.add(node)
        
        print(f"   - Extracted {len(unique_nodes)} unique concrete node types:")
        # æŒ‰ä¾†æºåˆ†çµ„é¡¯ç¤º
        for i, node in enumerate(unique_nodes[:50]):  # é¡¯ç¤ºå‰50å€‹
            sources = node_source_map.get(node, [])
            source_preview = sources[0][:50] + "..." if sources and len(sources[0]) > 50 else (sources[0] if sources else "Unknown")
            print(f"      {i+1}. {node} (from: {source_preview})")
        
        if len(unique_nodes) > 50:
            print(f"      ... and {len(unique_nodes) - 50} more nodes")
        
        initial_concrete_nodes = unique_nodes
        
        # å¦‚æœæ²’æœ‰ mapped_nodesï¼Œå˜—è©¦å¾é—œéµå­—æ¨æ–·
        if not initial_concrete_nodes:
            print(" - No mapped_nodes found, trying to infer from keywords...")
            for kw in keywords:
                # åœ¨ ontology ä¸­æŸ¥æ‰¾
                for node_type in self.ontology.keys():
                    if kw.lower() in node_type.lower() or any(kw.lower() in str(v).lower() for v in self.ontology[node_type].values()):
                        initial_concrete_nodes.append(node_type)
                        if len(initial_concrete_nodes) >= 3:
                            break
                if len(initial_concrete_nodes) >= 3:
                    break
        
        initial_concrete_nodes = list(set(initial_concrete_nodes))
        print(f" - Extracted {len(initial_concrete_nodes)} concrete node types")
        
        if not initial_concrete_nodes:
            print("âš ï¸  Warning: No concrete nodes extracted. Cannot generate workflow.")
            return []
        
        # STAGE 2: Workflow Composition
        print("\nSTAGE 2: Workflow Composition (A*)")
        candidates = self.composer.compose(
            matched_leaves=matched_leaves,
            initial_concrete_nodes=initial_concrete_nodes,
            user_query=user_query,
            params=extracted_params
        )
        
        if not candidates:
            print("âš ï¸  Warning: Failed to generate workflow candidates.")
            return []
        
        print(f" - Generated {len(candidates)} workflow candidates")
        
        return candidates
    
    def _build_categories_from_taxonomy(self, taxonomy_file_path: str) -> Dict:
        """
        å¾ taxonomy çš„ç¬¬ä¸€å±¤ï¼ˆé ‚å±¤åˆ†é¡ï¼‰æ§‹å»º function categories
        
        ä½¿ç”¨ç¬¬ä¸€å±¤åˆ†é¡æ›´å®¹æ˜“ç²å¾— category åˆ†æ•¸ï¼Œå› ç‚ºï¼š
        - ç¬¬ä¸€å±¤åˆ†é¡æ›´å¯¬æ³›ï¼Œæ›´å®¹æ˜“åŒ¹é…
        - ä¾‹å¦‚ï¼š"1 Commerce & Revenue Operations", "2 Customer Engagement & Marketing"
        """
        import json
        categories = {}
        try:
            with open(taxonomy_file_path, 'r', encoding='utf-8') as f:
                raw_taxonomy = json.load(f)
            
            # ç²å– Taxonomy æ ¹ç¯€é»
            taxonomy_root = raw_taxonomy.get("Taxonomy", raw_taxonomy)
            
            # ç›´æ¥æå–ç¬¬ä¸€å±¤ï¼ˆé ‚å±¤åˆ†é¡ï¼‰
            for top_key, top_value in taxonomy_root.items():
                if isinstance(top_value, dict):
                    # æå–ä¹¾æ·¨çš„åç¨±ï¼ˆå»æ‰æ•¸å­—å‰ç¶´ï¼‰
                    # ä¾‹å¦‚ï¼š"1 Commerce & Revenue Operations" -> "Commerce & Revenue Operations"
                    # ä¾‹å¦‚ï¼š"2 Customer Engagement & Marketing" -> "Customer Engagement & Marketing"
                    parts = top_key.split(' ', 1)  # åˆ†å‰²æ•¸å­—å’Œåç¨±
                    if len(parts) == 2 and parts[0].isdigit():
                        clean_name = parts[1]  # æå–åç¨±éƒ¨åˆ†
                    else:
                        clean_name = top_key  # å¦‚æœæ ¼å¼ä¸å°ï¼Œä½¿ç”¨åŸå§‹åç¨±
                    
                    # ç²å–æè¿°
                    description = top_value.get("Description", top_value.get("description", clean_name))
                    
                    # æ·»åŠ åˆ° categories
                    categories[clean_name] = description
            
            if not categories:
                print("   âš ï¸  Warning: Could not build categories. Taxonomy structure might be unexpected.")
                return {"Default": "Default category"}  # å‚™æ´
            
            # èª¿è©¦ï¼šè¼¸å‡ºå‰å¹¾å€‹æå–çš„ categories
            print(f"   - Sample categories extracted (first 10):")
            for i, (cat_name, cat_desc) in enumerate(list(categories.items())[:10]):
                print(f"      {i+1}. {cat_name}: {cat_desc[:60]}...")
            
            return categories
            
        except Exception as e:
            print(f"   âš ï¸  Error loading taxonomy for categories: {e}")
            return {
                "Error": "Could not load taxonomy categories dynamically."
            }

